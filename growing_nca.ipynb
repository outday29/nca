{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660ddf07-0a8a-40e9-b51d-45320763b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# This will reload all modules every time you run a cell\n",
    "# You may refer to here https://switowski.com/blog/ipython-autoreload/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c41f71-839e-4874-bcbf-93971d5fd7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.rmtree(\"./lightning_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88b66dcb-967d-4356-a24f-69dfdab5825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from torch import optim\n",
    "\n",
    "from utils import VideoWriter, transform\n",
    "from display import NCAGrid\n",
    "from lightning_module import NCALightningModule\n",
    "from dataset import NCADataModule\n",
    "from model import Updater, Perceiver, NCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e83e810-c2e7-44fd-80dd-cd2d0f840665",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_CACHE = Path(\"./seed\") # For storing all seed caches\n",
    "SEED_CACHE_SIZE = 16 # Must be at least the batch_size, to avoid the drop_last, Might not be neccessary tho\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_STEP = 56\n",
    "\n",
    "SEED_CACHE.mkdir(exist_ok=True, parents=True)\n",
    "GRID_SIZE = (40, 40)\n",
    "CELL_FIRE_RATE = 0.5\n",
    "CLIP_VALUE = [-10, 10]\n",
    "ALIVE_THRESHOLD = 0.1\n",
    "USE_ALIVE_CHANNEL = True # If False, all cells are assume alive\n",
    "THUMBNAIL_SIZE = 32 # This controls the size of the target image, must be smaller than grid_size\n",
    "NUM_HIDDEN_CHANNELS = 18\n",
    "NUM_STATIC_CHANNELS = 0\n",
    "NUM_TARGET_CHANNELS = 3\n",
    "TOTAL_CHANNELS = NUM_HIDDEN_CHANNELS + NUM_STATIC_CHANNELS + NUM_TARGET_CHANNELS + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ad1a047",
   "metadata": {},
   "source": [
    "Defining our NCA dataset object, each NCA data in the dataset defines the state of the grid.\n",
    "\n",
    "Below are the explanation for each arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6221b2b8-f4c6-47c8-a0f8-05f4724327f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_dm = NCADataModule(\n",
    "    seed_cache_dir=SEED_CACHE, \n",
    "    grid_size=GRID_SIZE, \n",
    "    num_hidden_channels=NUM_HIDDEN_CHANNELS, \n",
    "    num_target_channels=NUM_TARGET_CHANNELS, \n",
    "    num_static_channels=NUM_STATIC_CHANNELS, \n",
    "    target_image_path=\"./pic/32/emoji_u0037_20e3.png\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    thumbnail_size=THUMBNAIL_SIZE,\n",
    "    clear_cache=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d396c7f1",
   "metadata": {},
   "source": [
    "Visualizing the target emoji that our NCA has to form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ea50fc-f94b-4c56-842c-bcbc55adcd7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not save to JPEG for display",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mD:\\nca\\nca\\venv\\lib\\site-packages\\PIL\\JpegImagePlugin.py:639\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 639\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m \u001b[43mRAWMODE\u001b[49m\u001b[43m[\u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'RGBA'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mD:\\nca\\nca\\venv\\lib\\site-packages\\PIL\\Image.py:643\u001b[0m, in \u001b[0;36mImage._repr_image\u001b[1;34m(self, image_format, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(b, image_format, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\nca\\nca\\venv\\lib\\site-packages\\PIL\\Image.py:2413\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2413\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\nca\\nca\\venv\\lib\\site-packages\\PIL\\JpegImagePlugin.py:642\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename)\u001b[0m\n\u001b[0;32m    641\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot write mode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mim\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as JPEG\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 642\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    644\u001b[0m info \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mencoderinfo\n",
      "\u001b[1;31mOSError\u001b[0m: cannot write mode RGBA as JPEG",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mD:\\nca\\nca\\venv\\lib\\site-packages\\IPython\\core\\formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\nca\\nca\\venv\\lib\\site-packages\\PIL\\Image.py:661\u001b[0m, in \u001b[0;36mImage._repr_jpeg_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_jpeg_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    657\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"iPython display hook support for JPEG format.\u001b[39;00m\n\u001b[0;32m    658\u001b[0m \n\u001b[0;32m    659\u001b[0m \u001b[38;5;124;03m    :returns: JPEG version of the image as bytes\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 661\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_repr_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJPEG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\nca\\nca\\venv\\lib\\site-packages\\PIL\\Image.py:646\u001b[0m, in \u001b[0;36mImage._repr_image\u001b[1;34m(self, image_format, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    645\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not save to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for display\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 646\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m b\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "\u001b[1;31mValueError\u001b[0m: Could not save to JPEG for display"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAYAAACM/rhtAAAFgklEQVR4Ae2Yz29bRRDHxz+TOLGdRG3SFtQUiR4qEHABCbV3LvwFPXJC5Y9o+4f0yAlu9MqhAnooPVQqqkAC1CiOIkVpEtuJ/exn+zHfmZ31s7PPSQ7cvJKz+/btznz2OzPPzyGat7kCcwVmKpAL3f3i/oNHn99/8LBQXqJRkvCHpOchDSmhZMR9ep7nhljDC2UN/0E/Ss2bDbHHTqXnSczHUZcaPz15vPv0yaNpniDgd0//SGrXblKuWBZD4lyA1KCHEwcOTu47xzy2PUMHoQcdz9vBcX/Y71Fnv0Evv717hqc4TYxrKHd5uLHSl4GDggkLQfgEWhBw+rQFPtdauaBhQeigFocPxk2J6V6VG68xaKzD+KA7oNjv53W5M+IJbgYgckydA+6r9xeplM8xEjf+E3U7MpbT8zXmRwnuAyjnwHVOwXNsL6HiQkV6wEeDhH74pyW9+OK8DrUMQDjSD5RLw520m1QfnlKlXJRCgFEAMpcM0ANUegeP62Y3pkavT+XlqhwAB6+X83Qaj7wv2JpuGYCToQMAKOBoEMd060qVNqqL07ZmXjeaHfp3+5SKbAcKDlyoTQikTKjlQ5OWX1atBsc25fShPefNQcGE88zDMSQeSwqYbTeoYOg5J3BMUanW6NftXaqW8u5Zx8pCXf7D/kRlXK8vL9C9D6567jf7bQ7vhirHC5F3JoAVlF+cGgQBbaOcEC7ZGHIYfb5YpLUbWwqDCdzjXuBc3zo+otvLBe9m57hDe70cLa+ogoAbOPUUjm1kFEk4xHDqDGCjwSEHAZKGAZxVPOZxuFx0Qnc2ah7w9V6TFuvrEl6DwzrdZ0ry5kDLAASIM+AUvCjcaatFt9cr3lUziunNUY+lLwgQlAOcKOjGFgG/KTUIAgImrcpF4eAoah3RZ++teRe/7xzSUm3tDBxCq6lkUbmEgkOElZ2NP+eHFWu7nS59vLlCC0U9dzQY0uv9E8ovLIlippxWMkKrdsUPkjnQggpOgyG3rEpxL62uqYBDNQ8P6KPNunfzF1cuVepBOAk1G1Z7qqLfmBrMAHTJy0YuAtfrRfThaplqiyVv/tnbd1SsVCWUBsTfcAIsOSiHdSGGCoGWAWiVNRmGLOWgYrt5TF9uXfEu/mT1ukX97g3ByRzvA5c+MfzWiUEmIDbaAxQAs+Bi/vpbLwyonlLv+baqlwWngAanBTNB5i7CgKhiJpzMRT1tOucUnF8EDt/R3Vtj9fbaEe32+JsGjxa2kw4rwOxjz1ocHmkUasFvEnk4m/wIgRuH4AaDEV0tJXRzdfzse/72gEorNVcc45wzMOsnIhOi47mwgh5OpccJQ3Co3HbziCu36s1H8ZBeHUTyhizqpRQzMPQxf9Jpw5fBNgNQ4bDRQg0gCyvG8XBIhd4pfXK97o3/wpVbWqlraHlzGsrGgMNY08jZh6NAywA0xXSzQE3BYa5zckL3ttYnzP62c0wFfnMWGPZpUNYbnCqoea7RmTDjLzIAXflLqHkcgMPp4/YxfXpj1Rt72Tii0ZLmXqgw0nAANrsQD1EKtXCR8MqssKqaWpkY//z3vryG4SX3RaNJ+fq1YNVOw8WcgKacVfPFAXE6do48O4gG1GU58BtiAo7XFNc26UUr1nzi9Ultg3+d5WeGFcoBLmLjrf5w/FoXouO5sIIODpDMRz/yry/8wLGk1pNDAYa2w8i4zwfruUJK3ZtIERyeBK7PBkQIvkYfahmAFmLtoWD61xeUhcG0cTg1cIwtbDiErtV+Oqz+PjYFWrBI8L+SAf87QgH0xB7o/4CL2Vefn52BFlQQ/8i5/vU3D6m86JQwFRiWjQisB3XXXCTpbyAoJW9BWD+9FuqbHX6WjuI+0bPvHwf45lNzBeYKnKfAfwfLy5BJ5uSfAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=40x40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "to_pil_func = T.ToPILImage()\n",
    "img = to_pil_func(lit_dm.dataset.target_image_processed)\n",
    "img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5679750",
   "metadata": {},
   "source": [
    "Defining our NCA module. A NCA model will take in the current grid state provided by the DataModule object and output the next grid state.\n",
    "\n",
    "Every NCA module will have a `Perceiver` object defining how each individual cell in the grid receive the state information of its neighbouring cells, and an `Updater` object defining how to compute the change in cell state from the data received from `Perceiver`.\n",
    "\n",
    "\n",
    "Below are the explanation for each of the arguments:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e9181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NET_HIDDEN_CHANNELS = 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6e1770-97b3-4571-b35d-3e16ca9ebfb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceiver(\n",
       "  (model): Conv2d(22, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceiver_net = Perceiver(in_channels=TOTAL_CHANNELS, out_channels=NET_HIDDEN_CHANNELS, groups=1)\n",
    "perceiver_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99abf91c-0dce-4079-957b-10c73f4ccc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Updater(\n",
       "  (out): Sequential(\n",
       "    (0): Conv2d(63, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 22, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updater_net = Updater(in_channels=NET_HIDDEN_CHANNELS, out_channels=TOTAL_CHANNELS)\n",
    "updater_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62cac612-48a3-4eaf-8270-5af1d9285348",
   "metadata": {},
   "outputs": [],
   "source": [
    "nca_2d = NCA(\n",
    "    num_hidden_channels = NUM_HIDDEN_CHANNELS,\n",
    "    num_target_channels = NUM_TARGET_CHANNELS,\n",
    "    num_static_channels = NUM_STATIC_CHANNELS,\n",
    "    use_alive_channel = USE_ALIVE_CHANNEL,\n",
    "    perceiver = perceiver_net,\n",
    "    updater= updater_net,\n",
    "    cell_fire_rate = CELL_FIRE_RATE,\n",
    "    clip_value = CLIP_VALUE,\n",
    "    alive_threshold = ALIVE_THRESHOLD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5586ac38-0697-4306-9809-0200579f6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = NCALightningModule(\n",
    "    model = nca_2d,\n",
    "    train_step = TRAIN_STEP,\n",
    "    seed_cache_dir = SEED_CACHE,\n",
    "    seed_cache_size = SEED_CACHE_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7d86bb1-4733-4471-9fa0-cd37d98fffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = NCALightningModule.load_from_checkpoint(\n",
    "    \"./tb_logs/lightning_logs/version_11/checkpoints/epoch=395-step=1585.ckpt\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cf893c4",
   "metadata": {},
   "source": [
    "After defining our grid and the NCA model, we are ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "888a21f9-040f-4195-98b7-02ee95f6c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_module.callback import get_num_generator, CacheBestSeed, CacheCorruptedSeed, VisualizeBestSeed, VisualizeRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2058833f-0b4e-4382-b1aa-ba278ae5b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_func = transform.create_corrupt_2d_circular(h=GRID_SIZE[0], w=GRID_SIZE[1], radius=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53d7c6bc-eee5-440e-b1ed-3fd01be921ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\")\n",
    "\n",
    "num_gen = get_num_generator(SEED_CACHE_SIZE)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1000,\n",
    "    reload_dataloaders_every_n_epochs=1, # Since the seed cache is updated every epoch\n",
    "    callbacks=[\n",
    "        CacheBestSeed(cache_dir=SEED_CACHE, num_generator=num_gen),\n",
    "        VisualizeBestSeed(),\n",
    "        VisualizeRun(interval=30, simulate_step=TRAIN_STEP),\n",
    "        CacheCorruptedSeed(cache_dir=SEED_CACHE, num_generator=num_gen, loss_threshold=0.15, corrupt_func=corrupt_func)\n",
    "    ],\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87360431-071c-4e86-a1f3-f6f96e5db3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | NCA     | 22.1 K\n",
      "1 | loss  | MSELoss | 0     \n",
      "----------------------------------\n",
      "22.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "22.1 K    Total params\n",
      "0.089     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163:   0%|                                    | 0/4 [1:36:26<?, ?it/s, loss=0.0106, v_num=12]\n",
      "Epoch 242:   0%|                                     | 0/4 [00:00<?, ?it/s, loss=0.00706, v_num=12]"
     ]
    }
   ],
   "source": [
    "trainer.fit(lit_model, lit_dm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16542da5-8246-4faa-af4b-43715e29c284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d85f58-6fc2-46e2-a696-847042277d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccbe209-78f1-42a3-b4e4-78435bc65a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
